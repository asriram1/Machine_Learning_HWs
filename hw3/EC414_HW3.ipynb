{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 3: Feature Engineering, KNN, and Decision Trees\n",
    "\n",
    "By Rachel Manzelli and Brian Kulis with the help of N. Frumkin, K. Chauhan, and A. Tsiligkaridis\n",
    "\n",
    "\n",
    "*Wine classification dataset from the UCI Machine Learning Repository: http://archive.ics.uci.edu/ml/datasets/Wine*\n",
    "\n",
    "*Sea level dataset from NASA's climate change repository: https://climate.nasa.gov/vital-signs/*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Preamble\n",
    "\n",
    "Download the `hw3` folder from here: https://github.com/nfrumkin/EC414/tree/master/homework/hw3 (or clone the EC414 repository, if you prefer).\n",
    "\n",
    "To run and solve this assignment, you must have a working Jupyter Notebook installation.\n",
    "\n",
    "If you followed the installation instructions for `Python 3.6.x` and `Jupyter Notebook` from discussion 1, you should be set. In a terminal (cmd or Powershell for Windows users), navigate to the `hw3` folder. Then type `jupyter notebook` and press `Enter`.\n",
    "\n",
    "If you have Anaconda, run Anaconda and choose this file (`EC414_HW3.ipynb`) in Anaconda's file explorer. Use `Python 3` version.\n",
    "\n",
    "Below statements assume that you have already followed these instructions. If you need help with Python syntax, NumPy, or Matplotlib, you might find [Week 1 discussion material](https://github.com/nfrumkin/EC414/blob/master/discussions/Week%201%20-%20Python%20Review.ipynb) useful. \n",
    "\n",
    "To run code in a cell or to render [Markdown](https://en.wikipedia.org/wiki/Markdown)+[LaTeX](https://en.wikipedia.org/wiki/LaTeX) press `Ctrl+Enter` or `[>|]` (\"play\") button above. To edit any code or text cell [double] click on its content. To change cell type, choose \"Markdown\" or \"Code\" in the drop-down menu above.\n",
    "\n",
    "Put your solution into boxes marked with **`[double click here to add a solution]`** and press Ctrl+Enter to render text. [Double] click on a cell to edit or to see its source code. You can add cells via **`+`** sign at the top left corner.\n",
    "\n",
    "Submission instructions: please upload your completed solution file(s) to Blackboard by the due date (see Schedule)."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Feature Engineering\n",
    "\n",
    "Given the sea level rise data over the past 25 years as a training set (`sealevel_train.csv`) and the past year's sea level rise data as a testing set (`sealevel_test.csv`):\n",
    "\n",
    "**a.** Apply Ordinary Least Squares regression on the training data. Plot both the training data and the regression curve on the same figure. *(Hint: use your code from Homework 2!)*\n",
    "\n",
    "**b.** Using the testing set, calculate the mean squared error (MSE) between the ground truth testing data and the prediction given by your regression curve from part a. Recall that the mean squared error is given by $MSE = \\frac{1}{n}\\sum_{i=1}^{n}(y_i - \\hat{y_i})^2$, where $y$ is the vector of $n$ observed values and $\\hat{y}$ is the vector of $n$ predictions.\n",
    "\n",
    "**c.** In machine learning, we often manipulate the raw data into some intermediary form to create a more robust feature representation. Repeat parts a. and b. for each of the following data transformations. *Be sure to apply the same transformation to the testing set before finding the MSE between the test data and the prediction.*\n",
    "\n",
    "* Apply a rolling mean with a window size of 5, 7, and 15. This transformation is given by $x_j = \\sum_{k=i}^{i+window size}(\\frac{x_k}{windowsize})$. With the $n$ data points given, pad the final values with zero. (For example, the mean at $n-2$ includes points $n-2$, $n-1$, and $n$, and will be 0 for $n+1$ and $n+2$ because we do not have data for those points.)\n",
    "\n",
    "* Apply first order differencing. The transformation is given by $x_{j} = x_i - x_{i-1}$ for every data point $x_i$ in the dataset, excluding the first data point.\n",
    "\n",
    "* One other data transformation of your choice. *Be sure to explain your transformation.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "from sklearn.metrics import mean_squared_error, mean_absolute_error\n",
    "from sklearn.linear_model import LinearRegression"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>level_variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>1993.011526</td>\n",
       "      <td>-37.52</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>1993.038692</td>\n",
       "      <td>-38.05</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>1993.065858</td>\n",
       "      <td>-37.61</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>1993.093025</td>\n",
       "      <td>-37.49</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>1993.120191</td>\n",
       "      <td>-36.48</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          time  level_variation\n",
       "0  1993.011526           -37.52\n",
       "1  1993.038692           -38.05\n",
       "2  1993.065858           -37.61\n",
       "3  1993.093025           -37.49\n",
       "4  1993.120191           -36.48"
      ]
     },
     "execution_count": 2,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read training set\n",
    "sea_level_df = pd.read_csv(\"sealevel_train.csv\")\n",
    "sea_level_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>time</th>\n",
       "      <th>level_variation</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>2013.453940</td>\n",
       "      <td>26.31</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>2013.481106</td>\n",
       "      <td>26.32</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>2013.508272</td>\n",
       "      <td>26.44</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>2013.535439</td>\n",
       "      <td>26.54</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>2013.562605</td>\n",
       "      <td>26.49</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "          time  level_variation\n",
       "0  2013.453940            26.31\n",
       "1  2013.481106            26.32\n",
       "2  2013.508272            26.44\n",
       "3  2013.535439            26.54\n",
       "4  2013.562605            26.49"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Read testing set\n",
    "sea_level_df_test = pd.read_csv(\"sealevel_test.csv\")\n",
    "sea_level_df_test.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part a - Fit linear regression model to training data (find OLS coefficients) #\n",
    "\n",
    "\n",
    "# Predict using OLS model\n",
    "\n",
    "\n",
    "# Plot training data along with the regression curve"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part b - Prediction of testing points #\n",
    "\n",
    "# Calculate mean squared error between ground truth and predictions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c - data transformations #\n",
    "# ROLLING MEAN\n",
    "\n",
    "# Repeat parts a and b for rolling mean"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c - data transformations #\n",
    "# FIRST ORDER DIFFERENCING\n",
    "\n",
    "# Repeat parts a and b for first order differencing"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Part c - data transformations #\n",
    "# YOUR OWN TRANSFORMATION\n",
    "\n",
    "# Repeat parts a and b for your own transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: K-Nearest Neighbors\n",
    "\n",
    "Consider the following five two-dimensional training points, belonging to class + or class -:\n",
    "\n",
    "$(0,1,+)$\n",
    "\n",
    "$(1,1,-)$\n",
    "\n",
    "$(2,2,+)$\n",
    "\n",
    "$(2,0,+)$\n",
    "\n",
    "$(3,1,-)$\n",
    "\n",
    "**a.** Plot these five points. Then, draw the decision boundary for a **1-nearest-neighbor classifier (with Euclidean distance).** *Be sure to show or explain how you found your decision boundary.*\n",
    "\n",
    "**b.** Classify the following test points, and add them to your plot:\n",
    "\n",
    "$(0,0)$\n",
    "\n",
    "$(1,2)$\n",
    "\n",
    "$(2,1)$\n",
    "\n",
    "$(3,3)$\n",
    "\n",
    "$(4,3)$\n",
    "\n",
    "*Be sure to explain how you classified the test points.*\n",
    "\n",
    "**c.** As you may or may not have seen in part b, it is possible for two neighbors with different class labels to have identical distances to a test point. In that case, explain how to choose a class for this point."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Nearest Neighbors vs. Decision Trees\n",
    "\n",
    "Not all machine learning algorithms perform well on all types of data; performance is often dependent on how the data is distributed. We ask you to compare the performance accuracies of k-nearest neighbors and decision trees on two datasets: the Iris dataset and the wine dataset.\n",
    "\n",
    "The Iris flower dataset contains samples of attributes of 3 different variations of Iris flowers. This dataset has become very commonly used in training classification models. In fact, it has become so common that it exists within scikit-learn. The wine dataset contains attributes of 3 different types of wine. The datasets are imported and split into training and testing sets below for you. You can also get a preview of what they look like by running the two cells below.\n",
    "\n",
    "**a.** Using the `KNeighborsClassifier` and `DecisionTreeClassifier` from scikit-learn, train and test k-nearest neighbors and decision trees on both datasets. When training the k-nearest neighbor algorithm, **do not choose a random number of neighbors, but instead *find the k that achieves the best accuracy in the range 1-10***. Compute the accuracies of each method (hint: you can use scikit-learn's `metrics.accuracy_score`).\n",
    "\n",
    "**b.** Compare the accuracies of each algorithm on each dataset. **If there is a difference in their performance, why do you think this would be?**\n",
    "\n",
    "*Please note that since sampling of data is random, accuracies may differ when you run the code again, and they will differ among your peers.*"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>sepal length (cm)</th>\n",
       "      <th>sepal width (cm)</th>\n",
       "      <th>petal length (cm)</th>\n",
       "      <th>petal width (cm)</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>5.1</td>\n",
       "      <td>3.5</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>4.9</td>\n",
       "      <td>3.0</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>4.7</td>\n",
       "      <td>3.2</td>\n",
       "      <td>1.3</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>4.6</td>\n",
       "      <td>3.1</td>\n",
       "      <td>1.5</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>5.0</td>\n",
       "      <td>3.6</td>\n",
       "      <td>1.4</td>\n",
       "      <td>0.2</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "   sepal length (cm)  sepal width (cm)  petal length (cm)  petal width (cm)\n",
       "0                5.1               3.5                1.4               0.2\n",
       "1                4.9               3.0                1.4               0.2\n",
       "2                4.7               3.2                1.3               0.2\n",
       "3                4.6               3.1                1.5               0.2\n",
       "4                5.0               3.6                1.4               0.2"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Import Iris dataset from scikit-learn, as well as algorithms\n",
    "from sklearn.datasets import load_iris\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn import metrics\n",
    "\n",
    "from sklearn.neighbors import KNeighborsClassifier\n",
    "from sklearn import tree\n",
    "\n",
    "# Load Iris dataset\n",
    "iris = load_iris()\n",
    "iris_df = pd.DataFrame(iris.data, columns = iris.feature_names)\n",
    "\n",
    "# Split data into training and testing sets\n",
    "X = iris.data\n",
    "y = iris.target\n",
    "\n",
    "X_iris_train, X_iris_test, y_iris_train, y_iris_test = train_test_split(X, y, test_size = 0.3) # here, our test set is 30% of whole set\n",
    "\n",
    "# Show what Iris data looks like\n",
    "iris_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>alc</th>\n",
       "      <th>malic acid</th>\n",
       "      <th>ash</th>\n",
       "      <th>alcalinity</th>\n",
       "      <th>mg</th>\n",
       "      <th>phenols</th>\n",
       "      <th>flavanoid</th>\n",
       "      <th>nonflav phenols</th>\n",
       "      <th>proanth</th>\n",
       "      <th>color</th>\n",
       "      <th>hue</th>\n",
       "      <th>OD280/OD315</th>\n",
       "      <th>proline</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>14.23</td>\n",
       "      <td>1.71</td>\n",
       "      <td>2.43</td>\n",
       "      <td>15.6</td>\n",
       "      <td>127</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.06</td>\n",
       "      <td>0.28</td>\n",
       "      <td>2.29</td>\n",
       "      <td>5.64</td>\n",
       "      <td>1.04</td>\n",
       "      <td>3.92</td>\n",
       "      <td>1065</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>13.20</td>\n",
       "      <td>1.78</td>\n",
       "      <td>2.14</td>\n",
       "      <td>11.2</td>\n",
       "      <td>100</td>\n",
       "      <td>2.65</td>\n",
       "      <td>2.76</td>\n",
       "      <td>0.26</td>\n",
       "      <td>1.28</td>\n",
       "      <td>4.38</td>\n",
       "      <td>1.05</td>\n",
       "      <td>3.40</td>\n",
       "      <td>1050</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>13.16</td>\n",
       "      <td>2.36</td>\n",
       "      <td>2.67</td>\n",
       "      <td>18.6</td>\n",
       "      <td>101</td>\n",
       "      <td>2.80</td>\n",
       "      <td>3.24</td>\n",
       "      <td>0.30</td>\n",
       "      <td>2.81</td>\n",
       "      <td>5.68</td>\n",
       "      <td>1.03</td>\n",
       "      <td>3.17</td>\n",
       "      <td>1185</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>14.37</td>\n",
       "      <td>1.95</td>\n",
       "      <td>2.50</td>\n",
       "      <td>16.8</td>\n",
       "      <td>113</td>\n",
       "      <td>3.85</td>\n",
       "      <td>3.49</td>\n",
       "      <td>0.24</td>\n",
       "      <td>2.18</td>\n",
       "      <td>7.80</td>\n",
       "      <td>0.86</td>\n",
       "      <td>3.45</td>\n",
       "      <td>1480</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>13.24</td>\n",
       "      <td>2.59</td>\n",
       "      <td>2.87</td>\n",
       "      <td>21.0</td>\n",
       "      <td>118</td>\n",
       "      <td>2.80</td>\n",
       "      <td>2.69</td>\n",
       "      <td>0.39</td>\n",
       "      <td>1.82</td>\n",
       "      <td>4.32</td>\n",
       "      <td>1.04</td>\n",
       "      <td>2.93</td>\n",
       "      <td>735</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "     alc  malic acid   ash  alcalinity   mg  phenols  flavanoid  \\\n",
       "0  14.23        1.71  2.43        15.6  127     2.80       3.06   \n",
       "1  13.20        1.78  2.14        11.2  100     2.65       2.76   \n",
       "2  13.16        2.36  2.67        18.6  101     2.80       3.24   \n",
       "3  14.37        1.95  2.50        16.8  113     3.85       3.49   \n",
       "4  13.24        2.59  2.87        21.0  118     2.80       2.69   \n",
       "\n",
       "   nonflav phenols  proanth  color   hue  OD280/OD315  proline  \n",
       "0             0.28     2.29   5.64  1.04         3.92     1065  \n",
       "1             0.26     1.28   4.38  1.05         3.40     1050  \n",
       "2             0.30     2.81   5.68  1.03         3.17     1185  \n",
       "3             0.24     2.18   7.80  0.86         3.45     1480  \n",
       "4             0.39     1.82   4.32  1.04         2.93      735  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# Load wine dataset\n",
    "wine_df = pd.read_csv(\"wine.csv\", names = [\"class\", \"alc\", \"malic acid\", \"ash\", \"alcalinity\", \"mg\", \"phenols\", \"flavanoid\", \"nonflav phenols\", \"proanth\", \"color\", \"hue\", \"OD280/OD315\", \"proline\"])\n",
    "\n",
    "# Split dataset into train and test sets\n",
    "X = wine_df\n",
    "y = wine_df.pop('class')\n",
    "\n",
    "X_wine_train, X_wine_test, y_wine_train, y_wine_test = train_test_split(X, y, test_size = 0.3)\n",
    "\n",
    "# Show what wine dataset looks like\n",
    "wine_df.head()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best accuracy (from testing k=1 to 10): 0.9555555555555556\n",
      "Decision tree accuracy: 0.9333333333333333\n"
     ]
    }
   ],
   "source": [
    "# IRIS DATASET #\n",
    "\n",
    "# Construct the nearest neighbors classifier.\n",
    "# Fit the model to the data, and find the k which achieves the best accuracy in the range 1-10.\n",
    "# Test KNN with testing set\n",
    "\n",
    "# Construct a decision tree on the training data.\n",
    "# Test decision tree with testing set\n",
    "\n",
    "# Compare accuracies between the two algorithms (print them out)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "KNN best accuracy (from testing k=1 to 10): 0.7407407407407407\n",
      "Decision tree accuracy: 0.9074074074074074\n"
     ]
    }
   ],
   "source": [
    "# WINE DATASET #\n",
    "\n",
    "# Construct the nearest neighbors classifier.\n",
    "# Fit the model to the data, and find the k which achieves the best accuracy in the range 1-10.\n",
    "# Test KNN with testing set\n",
    "\n",
    "# Construct a decision tree on the training data\n",
    "# Test decision tree with testing set\n",
    "\n",
    "# Compare accuracies between the two algorithms (print them out)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part b]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 4: Curse of Dimensionality and Nearest Neighbors\n",
    "\n",
    "**Please note that you may find it easier to program this question in MATLAB. If you choose to use MATLAB, please submit a separate .m file with your code solution and either insert your plots into Jupyter (or, submit a separate document with the plots through Blackboard).**\n",
    "\n",
    "Consider $n$ data points uniformly distributed in a $p$-dimensional unit ball centered at the origin, and suppose we are interested in nearest neighbors to the origin. It can be shown that the median distance from the origin to the closest data point under this scenario is given by the expression $(1-(\\frac{1}{2})^{\\frac{1}{n}})^{\\frac{1}{p}}$. \n",
    "\n",
    "Now consider the following alternative scenario. Suppose $n$ data points are chosen uniformly\n",
    "from $[−1, 1]^p$ (the interval $[−1, 1]$ in p dimensions). Now consider the nearest neighbor to a point at the origin in terms of the $l_∞$ norm: $\\|x − y\\|_∞ = max_i|x_i − y_i|$.\n",
    "\n",
    "**a.** Write a piece of code that generates $n$ data points in $p$ dimensions distributed uniformly\n",
    "in $[−1, 1]^p$, and computes the $l_∞$ nearest neighbors to the origin. For $n = 5$, $n = 50$, and $n = 500$, plot the nearest neighbor distances as a function of $p$, for $p = 1$ to $200$. On the same plot(s), also show the curves corresponding to the median distance expression given above.\n",
    "\n",
    "**b.** What do you observe about the relationship between the formula and the $l_∞$ nearest neighbor distances? *Discuss and interpret all of your plots.*\n",
    "\n",
    "**c. *Bonus*** Prove a relationship between the ∞ nearest neighbor distances and the above formula."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot n p-dimensional uniformly-distributed data points\n",
    "\n",
    "\n",
    "# Compute nearest neighbors to the origin in terms of maximum norm\n",
    "\n",
    "\n",
    "# Plot nearest neighbor distances as a function of p = 1:200 for n = 5, 50, 500. Plot curves \n",
    "# corresponding to the median distance expression on the same figure."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add plots, if you programmed your solution outside Jupyter]`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.2"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
