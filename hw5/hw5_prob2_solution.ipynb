{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 5: Logistic Regression and Support Vector Machines\n",
    "\n",
    "by Natalia Frumkin and Karanraj Chauhan with help from B. Kulis, R. Manzelli, and A. Tsiligkardis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: SVM Toy Example\n",
    "\n",
    "Given the following two-class data set:\n",
    "\n",
    "**Class -1: **\n",
    "A = (1,1)\n",
    "B = (2,3)\n",
    "\n",
    "**Class +1: **\n",
    "C = (2,5)\n",
    "D = (4,2)\n",
    "\n",
    "<ol type=\"a\">\n",
    "  <li>Plot the data.</li>\n",
    "  <li>Plot the hyperplane described by w = $(3,2)^T, b = -12$</li>\n",
    "  <li>Calculate the $l_2$ distance of data point C from the hyperplane.</li>\n",
    "  <li>Determine if the hyperplane linearly separates the data. Explain.</li>\n",
    "  <li>Calculate the hard margin SVM hyperplane in canonical form.</li>\n",
    "  <li>Which, if any, data points lie on the SVM hyperplane?</li>\n",
    "</ol>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Logistic Regression"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<p>In this problem, we will use a logistic regression model to classify emails as \"spam\" (1) or \"non-spam\" (0). Recall that the hypothesis/decision rule in a logistic regression model is given by</p>\n",
    "\n",
    "$$h_\\theta(x) = \\sigma(\\theta^Tx) \\\\ \\text{where } \\sigma  \\text{ is the sigmoid function}$$\n",
    "\n",
    "<p>Since logistic regression does not have a closed form solution, we will use gradient descent to obtain the parameters $\\theta$. We will use the negative log likelihood loss with L2 regularization as the loss function. Mathematically, the loss function $l(\\theta)$ for a given set of parameters $\\theta$ will be,</p>\n",
    "\n",
    "$$l(\\theta) = NLL(\\theta) + \\frac{\\lambda}{2}||\\theta||^2 \\\\ \\text{where } NLL(\\theta) = -\\sum_{i=1}^{n} y_i\\log(h(x_i)) + (1 - y_i)\\log(1 - h(x_i))$$\n",
    "\n",
    "<p>The good news is, you won't have to worry about these equations for implementing gradient descent (hurray!). However, what you will need is the gradient or the derivative of the loss function. For a given $n$$ x $$d$ matrix $X$ of data, $n$ x $1$ vector of labels (0/1) $y$, and corresponding $n$ x $1$ vector of predictions $\\hat{y}$, the loss function gradient is</p>\n",
    "\n",
    "$$\\nabla l(\\theta) = (\\hat{y} - y)^{T} \\cdot X + \\lambda \\cdot \\theta$$\n",
    "\n",
    "<ol type=\"a\">\n",
    "    <li>Load the dataset file spambase_data.csv using pandas, and then split the dataset into a train set and a test set. Note: train/test ratio of 0.8/0.2 has been known to work, but you are welcome to try other values.</li>\n",
    "    <li>Using the loss gradient equation above, implement gradient descent (use only the train set for this) to find the parameters $\\theta$ of the logistic regression model. Note: $learning$ $rate = 0.00001$, $\\lambda$ = $10$, and $number$ $of$ $steps = 3000$ have been known to give a decent accuracy but you are welcome to try other values, especially for $number$ $of$ $steps$.</li>\n",
    "    <li>Report the correct classification rate (CCR) of the model on train data and test data. The CCR is defined as $$CCR = \\frac{num\\_correct\\_predictions}{num\\_samples}$$</li>   \n",
    "</ol>"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.model_selection import train_test_split\n",
    "\n",
    "from matplotlib import pyplot as plt\n",
    "\n",
    "import pdb"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>0</th>\n",
       "      <th>1</th>\n",
       "      <th>2</th>\n",
       "      <th>3</th>\n",
       "      <th>4</th>\n",
       "      <th>5</th>\n",
       "      <th>6</th>\n",
       "      <th>7</th>\n",
       "      <th>8</th>\n",
       "      <th>9</th>\n",
       "      <th>...</th>\n",
       "      <th>48</th>\n",
       "      <th>49</th>\n",
       "      <th>50</th>\n",
       "      <th>51</th>\n",
       "      <th>52</th>\n",
       "      <th>53</th>\n",
       "      <th>54</th>\n",
       "      <th>55</th>\n",
       "      <th>56</th>\n",
       "      <th>57</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.32</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.778</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.756</td>\n",
       "      <td>61</td>\n",
       "      <td>278</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>0.21</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.50</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.14</td>\n",
       "      <td>0.28</td>\n",
       "      <td>0.21</td>\n",
       "      <td>0.07</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.94</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.132</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.372</td>\n",
       "      <td>0.180</td>\n",
       "      <td>0.048</td>\n",
       "      <td>5.114</td>\n",
       "      <td>101</td>\n",
       "      <td>1028</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>0.06</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.71</td>\n",
       "      <td>0.0</td>\n",
       "      <td>1.23</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.19</td>\n",
       "      <td>0.12</td>\n",
       "      <td>0.64</td>\n",
       "      <td>0.25</td>\n",
       "      <td>...</td>\n",
       "      <td>0.01</td>\n",
       "      <td>0.143</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.276</td>\n",
       "      <td>0.184</td>\n",
       "      <td>0.010</td>\n",
       "      <td>9.821</td>\n",
       "      <td>485</td>\n",
       "      <td>2259</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.137</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>0.31</td>\n",
       "      <td>0.63</td>\n",
       "      <td>...</td>\n",
       "      <td>0.00</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.0</td>\n",
       "      <td>0.135</td>\n",
       "      <td>0.000</td>\n",
       "      <td>0.000</td>\n",
       "      <td>3.537</td>\n",
       "      <td>40</td>\n",
       "      <td>191</td>\n",
       "      <td>1</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>5 rows Ã— 58 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "      0     1     2    3     4     5     6     7     8     9  ...    48  \\\n",
       "0  0.00  0.64  0.64  0.0  0.32  0.00  0.00  0.00  0.00  0.00  ...  0.00   \n",
       "1  0.21  0.28  0.50  0.0  0.14  0.28  0.21  0.07  0.00  0.94  ...  0.00   \n",
       "2  0.06  0.00  0.71  0.0  1.23  0.19  0.19  0.12  0.64  0.25  ...  0.01   \n",
       "3  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "4  0.00  0.00  0.00  0.0  0.63  0.00  0.31  0.63  0.31  0.63  ...  0.00   \n",
       "\n",
       "      49   50     51     52     53     54   55    56  57  \n",
       "0  0.000  0.0  0.778  0.000  0.000  3.756   61   278   1  \n",
       "1  0.132  0.0  0.372  0.180  0.048  5.114  101  1028   1  \n",
       "2  0.143  0.0  0.276  0.184  0.010  9.821  485  2259   1  \n",
       "3  0.137  0.0  0.137  0.000  0.000  3.537   40   191   1  \n",
       "4  0.135  0.0  0.135  0.000  0.000  3.537   40   191   1  \n",
       "\n",
       "[5 rows x 58 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# read in raw dataset\n",
    "df = pd.read_csv('spambase_data.csv')\n",
    "df.head()    # sanity check"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "# split into train and test sets\n",
    "X_train, X_test, y_train, y_test = train_test_split(df.iloc[:, :-1], df.iloc[:, -1], test_size=0.2)\n",
    "\n",
    "# convert pandas dataframe/series to numpy objects\n",
    "X_train = X_train.to_numpy()\n",
    "X_test = X_test.to_numpy()\n",
    "y_train = y_train.to_numpy()\n",
    "y_test = y_test.to_numpy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [],
   "source": [
    "# append column of 1's to X_train to account for bias\n",
    "X_train = np.hstack([X_train, np.ones(shape=(X_train.shape[0], 1))])\n",
    "X_test = np.hstack([X_test, np.ones(shape=(X_test.shape[0], 1))])\n",
    "\n",
    "# reshape y's\n",
    "y_train = y_train.reshape(-1, 1)\n",
    "y_test = y_test.reshape(-1, 1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [],
   "source": [
    "# utility functions - these are functions that will come in handy while coding the main part\n",
    "def sigmoid(x):\n",
    "    \"\"\"Computes sigmoid of elements of x\n",
    "\n",
    "    Does computation in a more numerically stable way by taking care of edge cases\n",
    "    i.e. x = inf and x = -inf\n",
    "\n",
    "    This implementation throws RuntimeWarnings. Use mysigmoid to avoid them.\n",
    "    \"\"\"\n",
    "    return np.where(x >= 0,\n",
    "                   1 / (1 + np.exp(-x)),\n",
    "                   np.exp(x) / (1 + np.exp(x)))\n",
    "\n",
    "def mysigmoid(x):\n",
    "    \"\"\"Computes sigmoid of elements of x\n",
    "\n",
    "    Does computation in a more numerically stable way by taking care of edge cases\n",
    "    i.e. x = inf and x = -inf\n",
    "\n",
    "    The above implementation throws RuntimeWarnings. This does not.\n",
    "    \"\"\"\n",
    "    s = np.empty_like(x)\n",
    "    for i in range(x.shape[0]):\n",
    "        xi = x[i, 0]\n",
    "        if xi > 0:\n",
    "            s[i, 0] = 1 / (1 + np.exp(-1*xi))\n",
    "        else:\n",
    "            s[i, 0] = np.exp(xi) / (1 + np.exp(xi))\n",
    "\n",
    "    return s\n",
    "\n",
    "\n",
    "def sigmoid_derivative(x):\n",
    "    \"\"\"Computes derivative of sigmoid of x\n",
    "    Uses the property : sigmoid derivative = sigmoid * (1 - sigmoid)\n",
    "    \"\"\"\n",
    "    s = mysigmoid(x)\n",
    "    return s * (1 - s)\n",
    "\n",
    "\n",
    "def nll_loss(y_true, y_pred):\n",
    "    \"\"\"Negative log likelihood function\n",
    "\n",
    "    Arguments:\n",
    "        y_true {ndarray} -- (num_elements, 1) shaped ndarray of true (binary) labels\n",
    "        y_pred {ndarray} -- (num_elements, 1) shaped ndarray of predicted labels (continuous values in [0, 1])\n",
    "\n",
    "    Returns:\n",
    "        [type] -- [description]\n",
    "    \"\"\"\n",
    "    return np.where(y_true == 1,\n",
    "                   np.log(y_pred),\n",
    "                   np.log(1 - y_pred)).mean(axis=0)\n",
    "\n",
    "\n",
    "def fit_log_reg(X, y, w_init=None, num_epochs=100, learning_rate=0.001, alpha=50, verbose=False):\n",
    "    \"\"\"Fits logistic regression model to given data using gradient descent\n",
    "    Assumes that loss being used is Negative Log Likelihood with L2 regularization\n",
    "\n",
    "    Arguments:\n",
    "        X {ndarray} -- (num_elements, num_features) shaped ndarray of data\n",
    "        y {ndarray} -- (num_elements, 1) shaped ndarray of binary (0/1) labels corresponding to data\n",
    "\n",
    "    Keyword Arguments:\n",
    "        num_epochs {int} -- number of iterations in gradient descent (default: {100})\n",
    "        learning_rate {float} -- step size in gradient descent (default: {0.001})\n",
    "        alpha {int} -- coefficient of l2 regularization (aka lambda) (default: {50})\n",
    "        verbose {bool} -- prints loss and misc info during training, if set to True (default: {False})\n",
    "\n",
    "    Returns:\n",
    "        w {ndarray} -- (1, num_features+1) shaped ndarray of coefficients for log. reg.\n",
    "    \"\"\"\n",
    "    # dataset meta data\n",
    "    num_elements, num_features = X.shape\n",
    "\n",
    "    # initialize w to small random values or if provided, some initial value\n",
    "    if w_init is not None and w_init.shape == (num_elements, 1):\n",
    "        w = w_init\n",
    "    else:\n",
    "        w = np.random.rand(1, num_features)\n",
    "\n",
    "    for ne in range(num_epochs):\n",
    "        # make predictions using current value of w\n",
    "        y_pred = mysigmoid(X @ w.transpose())\n",
    "\n",
    "        # calculate gradient of loss function\n",
    "        nll_grad = (y_pred - y).transpose() @ X\n",
    "        l2_grad = alpha * w\n",
    "        nll_l2_grad =  nll_grad + l2_grad\n",
    "\n",
    "        # print debug info every 250 steps, if in debug mode\n",
    "        if verbose and (ne%250 == 0):\n",
    "            print('nll_grad={:.5f} l2_grad={:.5f}'.format(np.abs(nll_grad).mean(), np.abs(l2_grad).mean()))\n",
    "            print('ccr = {:.5f}'.format(((y_pred > 0.5) == y).mean()))\n",
    "\n",
    "        # update w\n",
    "        w -= learning_rate * nll_l2_grad\n",
    "\n",
    "    return w"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "nll_grad=7488.90245 l2_grad=4.99014\n",
      "ccr = 0.39103\n",
      "nll_grad=12873.73662 l2_grad=7.37099\n",
      "ccr = 0.62609\n",
      "nll_grad=7269.70466 l2_grad=10.56381\n",
      "ccr = 0.50435\n",
      "nll_grad=7381.62166 l2_grad=14.21048\n",
      "ccr = 0.44891\n",
      "nll_grad=752.41887 l2_grad=16.39328\n",
      "ccr = 0.72663\n",
      "nll_grad=6815.51639 l2_grad=18.83591\n",
      "ccr = 0.61984\n",
      "nll_grad=7324.88864 l2_grad=21.68851\n",
      "ccr = 0.47255\n",
      "nll_grad=7306.85055 l2_grad=23.76304\n",
      "ccr = 0.48424\n",
      "nll_grad=7168.60319 l2_grad=25.40845\n",
      "ccr = 0.53641\n",
      "nll_grad=11815.59797 l2_grad=27.25652\n",
      "ccr = 0.64103\n",
      "nll_grad=7270.45241 l2_grad=29.07273\n",
      "ccr = 0.49946\n",
      "nll_grad=8886.49261 l2_grad=30.12855\n",
      "ccr = 0.67880\n"
     ]
    }
   ],
   "source": [
    "# fit logistic regression model\n",
    "w = fit_log_reg(X_train, y_train, num_epochs=3000, learning_rate=0.00001, alpha=10, verbose=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Test ccr = 0.67970, Train ccr = 0.69158\n"
     ]
    }
   ],
   "source": [
    "# predict on test data and train data\n",
    "y_pred_test = mysigmoid(X_test @ w.transpose()) >= 0.5\n",
    "y_pred_train = mysigmoid(X_train @ w.transpose()) >= 0.5\n",
    "\n",
    "# calculate CCR\n",
    "ccr_test = (y_pred_test == y_test).mean()\n",
    "ccr_train = (y_pred_train == y_train).mean()\n",
    "print('Test ccr = {:.5f}, Train ccr = {:.5f}'.format(ccr_test, ccr_train))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.6.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
