{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9: Bayesian Models and Neural Networks\n",
    "\n",
    "By Rachel Manzelli and Brian Kulis with the help of N. Frumkin, K. Chauhan, and A. Tsiligkaridis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Bayesian Models\n",
    "\n",
    "Paleobotanists estimate the moment in the past when a \n",
    "given species became extinct by taking cylindrical, vertical core \n",
    "samples below the earth's surface, and looking for the last\n",
    "occurrence of the species in the fossil record. This is measured in meters\n",
    "above the point $P$ at which the species was known to have first\n",
    "emerged.  \n",
    "\n",
    "Letting $\\{y_i, i = 1, \\ldots, n\\}$ denote a sample of\n",
    "such distances above $P$ at a random set of locations, the model can be represented as\n",
    "\n",
    "\\begin{equation*}\n",
    "(y_i | \\theta) \\sim \\mbox{Unif}(0,\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "In this model the unknown $\\theta > 0$ can be used to estimate\n",
    "the species extinction time through carbon dating.  *This problem is about Bayesian inference for $\\theta$.* ***Please explain the mathematical steps for each part of the problem. Points will be deducted if you do not show and explain your work!***\n",
    "\n",
    "**a.** Show that the likelihood may be written as\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\{y_1,\\ldots, y_n\\}|\\theta) = \n",
    "\\theta^{-n} I(\\theta \\geq \\max(y_1, \\ldots, y_n)),\n",
    "\\end{equation*}\n",
    "\n",
    "where $I$ is the **indicator function**, i.e. $I(A) = 1 $ if $A$ is true and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part a]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** The Pareto distribution (written $\\theta \\sim \\mbox{Pareto}(\\alpha,\\beta)$\n",
    "has density/PDF:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta) = \\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\alpha \\beta^\\alpha \\theta^{-(\\alpha + 1)} & \\mbox{if}\\ \\theta \\geq \\beta \\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array},\n",
    "\\right .\n",
    "\\end{equation*}\n",
    "where the normalizers $\\alpha, \\beta > 0$.\n",
    "\n",
    "With the likelihood viewed as a constant multiple of a\n",
    "density for $\\theta$, show that the likelihood corresponds to\n",
    "the $\\mbox{Pareto}(n-1,m)$ distribution, *where you will need to determine m.*\n",
    "\n",
    "Now, let the prior\n",
    "for $\\theta$ be taken to be $\\mbox{Pareto}(\\alpha,\\beta)$\n",
    "and derive the posterior distribution $p(\\theta | y)$.\n",
    "\n",
    "Is the Pareto conjugate to the uniform? As discussed in class, a likelihood and prior are conjugate pairs if the posterior distribution is in the same class (in this case, a Pareto distribution) as the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part b]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** In an experiment conducted in the Antarctic in the\n",
    "1980's to study a particular species of fossil ammonite,\n",
    "the following was a linearly rescaled version of the data\n",
    "obtained, in ascending order: \n",
    "\n",
    "$y = (0.4, 1.0, 1.5, 1.7, 2.0, 2.1, 3.1, 3.7, 4.3, 4.9)$.  \n",
    "\n",
    "Prior information equivalent to a Pareto prior with $(\\alpha,\\beta) = (2.5,4)$ was available. Plot the prior, likelihood, and posterior distributions arising from this data set on the same graph, and briefly discuss what this picture implies about the updating of information from prior to posterior in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part c]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [],
   "source": [
    "# If you choose to program the plot, please do so within Jupyter here.\n",
    "# Make sure to explain the plot, as indicated in the question!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Limitations of the Perceptron\n",
    "\n",
    "We aim to train a perceptron to model the logic functions **OR**$(x_1,x_2)$ and **XNOR**$(x_1, x_2)$, using the set of four 2D points, $x \\in \\{(0,0)^T, (1,0)^T, (0,1)^T, (1,1)^T\\}$.\n",
    "\n",
    "In order to model **OR**$(x_1,x_2)$, the perceptron classifier must output $1$ for $x \\in \\{(1,0)^T,(0,1)^T, (1,1)^T\\}$ and output $0$ if $x = (0,0)^T$. The perceptron classifier (activation threshold of the perceptron) is represented by $f(x) =\\mathbf 1[ w^Tx + b > 0]$.\n",
    "\n",
    "Instead of using a bias vector $b$, we can augment the data by $1$ and use a linear classifier: $f(x) =\\mathbf 1[ w^T x > 0 ]$. To do this, we replace $x$ with $x$ $\\in \\{(1,1,0)^T,(1,0,1)^T, (1,1,1)^T, (1,0,0)\\}$ and $w$ with a vector in $\\mathbb{R^3}$.\n",
    "\n",
    "**a.** Using the initial weight vector $w_0 = (0,0,0)^T$ and the [perceptron algorithm](https://www.cs.cmu.edu/~avrim/ML10/lect0125.pdf), derive the $w$ that models the **OR**$(x_1,x_2)$ function. **You can do this either manually (i.e. writing out the weight updates) or by programming the algorithm in Jupyter.** Before you begin, you should normalize your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part a, if handwritten]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight vector on epoch  0  sample  1 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  2 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  3 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  4 is  [-0.29289322  0.70710678  0.        ]\n",
      "Weight vector on epoch  1  sample  1 is  [-0.29289322  0.70710678  0.        ]\n",
      "Weight vector on epoch  1  sample  2 is  [0.41421356 0.70710678 0.70710678]\n",
      "Weight vector on epoch  1  sample  3 is  [0.41421356 0.70710678 0.70710678]\n",
      "Weight vector on epoch  1  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  1 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  2 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  3 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  1 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  2 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  3 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "The final weight vector to model OR(x1,x2) with this data is:  [-0.58578644  0.70710678  0.70710678]\n"
     ]
    }
   ],
   "source": [
    "# If you choose to program your algorithm, do so here. \n",
    "# Do not use sklearn except where we used it :).\n",
    "# We have started you off. Fill in places where we've written \"YOUR CODE HERE\".\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Step 1. Initialize weight vector & define data.\n",
    "x = np.array([[1, 1, 0], [1, 0, 1], [1, 1, 1], [1, 0, 0]]) # given data\n",
    "y = np.array([1, 1, 1, 0]) # correct predictions\n",
    "w = np.array([0, 0, 0])\n",
    "\n",
    "# Normalize each sample to have norm 1.\n",
    "x = normalize(x, norm='l2')\n",
    "\n",
    "# Step 2. Activation threshold (prediction).\n",
    "def predict(sample, weights):\n",
    "    ## YOUR CODE HERE: Return the prediction (1 or 0) based on the activation threshold\n",
    "    return None\n",
    "    ##\n",
    "\n",
    "# Step 3. Updating weights.\n",
    "def update(w, x, y, epochs):\n",
    "    for j in range(epochs):\n",
    "        i = 0\n",
    "        for sample in x:\n",
    "            # Make prediction using the above function\n",
    "            prediction = predict(sample, w)\n",
    "            \n",
    "            ## YOUR CODE HERE: Update weights according to the link above (pg. 2)\n",
    "            # Hint: how do we usually update weights? Use the true label somewhere...\n",
    "            w = None\n",
    "            ##\n",
    "            \n",
    "            i += 1\n",
    "            # This will help us see how often we make mistakes\n",
    "            print(\"Weight vector on epoch \", j, \" sample \", i, \"is \", w)\n",
    "    return w\n",
    "\n",
    "# Now, run the perceptron! Remember, you are done when the weight vector stabilizes.\n",
    "epochs = 4 # \"stop criteria\" - arbitrary, feel free to change as you see fit.\n",
    "w_new = update(w, x, y, epochs)\n",
    "print(\"The final weight vector to model OR(x1,x2) with this data is: \", w_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** We just showed that a perceptron can model **OR**$(x_1, x_2)$ successfully. Prove that a perceptron can't model **XNOR**$(x_1, x_2)$. *Hint: think about linearity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part b]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Neural Networks and the XNOR Problem\n",
    "\n",
    "So, the perceptron can't model the **XNOR**$(x_1, x_2)$ function. We now want to design a neural network (by hand) to solve the **XNOR** problem. \n",
    "\n",
    "**a.** Write the **XNOR** function in terms of the logical functions **OR**$(x_1,x_2)$, **AND**$(x_1,x_2)$, **NOR**$(x_1,x_2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part a]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** We will now design a network to model this function, using the hyperbolic tangent ([$tanh$](http://reference.wolfram.com/language/ref/Tanh.html)) as the activation function in all of the nodes. The network will take two binary variables as input, and output 1 only when the inputs are both 0 or both 1.\n",
    "\n",
    "The $tanh$ function outputs [-1,+1], not our desired output of [0,1]. Thus, we have appropriately changed the OR node to take +1/-1 as inputs. Also, we have added an extra last layer to convert the final output from +1/-1 to 0/1.\n",
    "\n",
    "*Hint: assume that $tanh$ outputs -1 for any input $x\\leq -2$, +1 for any input $x\\geq 2$, and 0 for $x=0$.*\n",
    "\n",
    "<img src=\"xnor1.png\" style=\"height:130px;\"><img src=\"xnor2.png\" style=\"height:110px;\">\n",
    "\n",
    "What are the missing weights $a,b,c,d,e,f$ of the **OR**, **NAND**, **AND** and **CONVERT** subnetworks, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part b]`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
