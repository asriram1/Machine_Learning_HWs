{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Homework 9: Bayesian Models and Neural Networks\n",
    "\n",
    "By Rachel Manzelli and Brian Kulis with the help of N. Frumkin, K. Chauhan, and A. Tsiligkaridis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 1: Bayesian Models\n",
    "\n",
    "Paleobotanists estimate the moment in the past when a \n",
    "given species became extinct by taking cylindrical, vertical core \n",
    "samples below the earth's surface, and looking for the last\n",
    "occurrence of the species in the fossil record. This is measured in meters\n",
    "above the point $P$ at which the species was known to have first\n",
    "emerged.  \n",
    "\n",
    "Letting $\\{y_i, i = 1, \\ldots, n\\}$ denote a sample of\n",
    "such distances above $P$ at a random set of locations, the model can be represented as\n",
    "\n",
    "\\begin{equation*}\n",
    "(y_i | \\theta) \\sim \\mbox{Unif}(0,\\theta)\n",
    "\\end{equation*}\n",
    "\n",
    "In this model the unknown $\\theta > 0$ can be used to estimate\n",
    "the species extinction time through carbon dating.  *This problem is about Bayesian inference for $\\theta$.* ***Please explain the mathematical steps for each part of the problem. Points will be deducted if you do not show and explain your work!***\n",
    "\n",
    "**a.** Show that the likelihood may be written as\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\{y_1,\\ldots, y_n\\}|\\theta) = \n",
    "\\theta^{-n} I(\\theta \\geq \\max(y_1, \\ldots, y_n)),\n",
    "\\end{equation*}\n",
    "\n",
    "where $I$ is the **indicator function**, i.e. $I(A) = 1 $ if $A$ is true and 0 otherwise."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part a]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** The Pareto distribution (written $\\theta \\sim \\mbox{Pareto}(\\alpha,\\beta)$\n",
    "has density/PDF:\n",
    "\n",
    "\\begin{equation*}\n",
    "p(\\theta) = \\left\\{\n",
    "\\begin{array}{cc}\n",
    "\\alpha \\beta^\\alpha \\theta^{-(\\alpha + 1)} & \\mbox{if}\\ \\theta \\geq \\beta \\\\\n",
    "0 & \\mbox{otherwise}\n",
    "\\end{array},\n",
    "\\right .\n",
    "\\end{equation*}\n",
    "where the normalizers $\\alpha, \\beta > 0$.\n",
    "\n",
    "With the likelihood viewed as a constant multiple of a\n",
    "density for $\\theta$, show that the likelihood corresponds to\n",
    "the $\\mbox{Pareto}(n-1,m)$ distribution, *where you will need to determine m.*\n",
    "\n",
    "Now, let the prior\n",
    "for $\\theta$ be taken to be $\\mbox{Pareto}(\\alpha,\\beta)$\n",
    "and derive the posterior distribution $p(\\theta | y)$.\n",
    "\n",
    "Is the Pareto conjugate to the uniform? As discussed in class, a likelihood and prior are conjugate pairs if the posterior distribution is in the same class (in this case, a Pareto distribution) as the prior."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part b]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**c.** In an experiment conducted in the Antarctic in the\n",
    "1980's to study a particular species of fossil ammonite,\n",
    "the following was a linearly rescaled version of the data\n",
    "obtained, in ascending order: \n",
    "\n",
    "$y = (0.4, 1.0, 1.5, 1.7, 2.0, 2.1, 3.1, 3.7, 4.3, 4.9)$.  \n",
    "\n",
    "Prior information equivalent to a Pareto prior with $(\\alpha,\\beta) = (2.5,4)$ was available. Plot the prior, likelihood, and posterior distributions arising from this data set on the same graph, and briefly discuss what this picture implies about the updating of information from prior to posterior in this case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part c]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "image/png": "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\n",
      "text/plain": [
       "<Figure size 432x288 with 1 Axes>"
      ]
     },
     "metadata": {
      "needs_background": "light"
     },
     "output_type": "display_data"
    }
   ],
   "source": [
    "# If you choose to program the plot, please do so within Jupyter here.\n",
    "# Make sure to explain the plot, as indicated in the question!\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.stats import pareto\n",
    "\n",
    "y = [0.4, 1.0, 1.5, 1.7, 2.0, 2.1, 3.1, 3.7, 4.3, 4.9]\n",
    "alpha = 2.5\n",
    "beta = 4\n",
    "x = np.linspace(0, 10, 100)\n",
    "\n",
    "\n",
    "\n",
    "y_prior = pareto.pdf(x, scale = beta, b = alpha)\n",
    "\n",
    "y_likelihood = pareto.pdf(x, scale = max(y), b = (len(y)-1))\n",
    "\n",
    "y_posterior = pareto.pdf(x, scale = max(beta,max(y)), b = (len(y)+alpha) )\n",
    "\n",
    "y_prior = [0]*100\n",
    "for i in range(100):\n",
    "    j = i/10\n",
    "    if(j>=beta):\n",
    "        y_prior[i]= alpha*(beta**alpha)*((1/j)**(alpha+1))\n",
    "        \n",
    "alpha = (len(y)-1)\n",
    "beta = max(y)\n",
    "\n",
    "y_likelihood = [0]*100\n",
    "for i in range(100):\n",
    "    j = i/10\n",
    "    if(j>=beta):\n",
    "        y_likelihood[i]= alpha*(beta**alpha)*((1/j)**(alpha+1))\n",
    "        \n",
    "alpha = (len(y)+alpha)\n",
    "beta = max(beta,max(y))\n",
    "\n",
    "y_posterior = [0]*100\n",
    "for i in range(100):\n",
    "    j = i/10\n",
    "    if(j>=beta):\n",
    "        y_posterior[i]= alpha*(beta**alpha)*((1/j)**(alpha+1))\n",
    "\n",
    "plt.plot(x, y_prior, 'r') # plotting t, a separately \n",
    "plt.plot(x, y_likelihood, 'b') # plotting t, b separately \n",
    "plt.plot(x, y_posterior, 'g') # plotting t, c separately \n",
    "\n",
    "plt.show()\n",
    "\n",
    "#The updated beliefs for theta calculated after updating from prior to posterior, shows us that a theta at 4.9 gives \n",
    "#the best possible likelihood distribution, as opposed to a theta of 4, as can be seen with the peaks of posterior, and \n",
    "#prior respectively. It can also be observed that the peak for the likelihood function is at 4.9, hence supporting the\n",
    "#results from the posterior suggesting a theta of 4.9 yields the best likelihood distribution. It can also be seen by the \n",
    "#graph that the posterior is very much determined by the likelihood, implying that the prior is uninformative. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 2: Limitations of the Perceptron\n",
    "\n",
    "We aim to train a perceptron to model the logic functions **OR**$(x_1,x_2)$ and **XNOR**$(x_1, x_2)$, using the set of four 2D points, $x \\in \\{(0,0)^T, (1,0)^T, (0,1)^T, (1,1)^T\\}$.\n",
    "\n",
    "In order to model **OR**$(x_1,x_2)$, the perceptron classifier must output $1$ for $x \\in \\{(1,0)^T,(0,1)^T, (1,1)^T\\}$ and output $0$ if $x = (0,0)^T$. The perceptron classifier (activation threshold of the perceptron) is represented by $f(x) =\\mathbf 1[ w^Tx + b > 0]$.\n",
    "\n",
    "Instead of using a bias vector $b$, we can augment the data by $1$ and use a linear classifier: $f(x) =\\mathbf 1[ w^T x > 0 ]$. To do this, we replace $x$ with $x$ $\\in \\{(1,1,0)^T,(1,0,1)^T, (1,1,1)^T, (1,0,0)\\}$ and $w$ with a vector in $\\mathbb{R^3}$.\n",
    "\n",
    "**a.** Using the initial weight vector $w_0 = (0,0,0)^T$ and the [perceptron algorithm](https://www.cs.cmu.edu/~avrim/ML10/lect0125.pdf), derive the $w$ that models the **OR**$(x_1,x_2)$ function. **You can do this either manually (i.e. writing out the weight updates) or by programming the algorithm in Jupyter.** Before you begin, you should normalize your data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part a, if handwritten]`**"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Weight vector on epoch  0  sample  1 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  2 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  3 is  [0.70710678 0.70710678 0.        ]\n",
      "Weight vector on epoch  0  sample  4 is  [-0.29289322  0.70710678  0.        ]\n",
      "Weight vector on epoch  1  sample  1 is  [-0.29289322  0.70710678  0.        ]\n",
      "Weight vector on epoch  1  sample  2 is  [0.41421356 0.70710678 0.70710678]\n",
      "Weight vector on epoch  1  sample  3 is  [0.41421356 0.70710678 0.70710678]\n",
      "Weight vector on epoch  1  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  1 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  2 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  3 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  2  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  1 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  2 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  3 is  [-0.58578644  0.70710678  0.70710678]\n",
      "Weight vector on epoch  3  sample  4 is  [-0.58578644  0.70710678  0.70710678]\n",
      "The final weight vector to model OR(x1,x2) with this data is:  [-0.58578644  0.70710678  0.70710678]\n"
     ]
    }
   ],
   "source": [
    "# If you choose to program your algorithm, do so here. \n",
    "# Do not use sklearn except where we used it :).\n",
    "# We have started you off. Fill in places where we've written \"YOUR CODE HERE\".\n",
    "from sklearn.preprocessing import normalize\n",
    "import numpy as np\n",
    "\n",
    "# Step 1. Initialize weight vector & define data.\n",
    "x = np.array([[1, 1, 0], [1, 0, 1], [1, 1, 1], [1, 0, 0]]) # given data\n",
    "y = np.array([1, 1, 1, 0]) # correct predictions\n",
    "w = np.array([0, 0, 0])\n",
    "\n",
    "# Normalize each sample to have norm 1.\n",
    "x = normalize(x, norm='l2')\n",
    "\n",
    "# Step 2. Activation threshold (prediction).\n",
    "def predict(sample, weights):\n",
    "    ## YOUR CODE HERE: Return the prediction (1 or 0) based on the activation threshold\n",
    "    \n",
    "    pred = np.dot(weights.T, sample)\n",
    "    if(pred>0):\n",
    "        prediction = 1\n",
    "        return prediction\n",
    "    else:\n",
    "        prediction = 0\n",
    "        return prediction\n",
    "        \n",
    "    #return None\n",
    "    ##\n",
    "\n",
    "# Step 3. Updating weights.\n",
    "def update(w, x, y, epochs):\n",
    "    for j in range(epochs):\n",
    "        i = 0\n",
    "        for sample in x:\n",
    "            # Make prediction using the above function\n",
    "            prediction = predict(sample, w)\n",
    "            \n",
    "            ## YOUR CODE HERE: Update weights according to the link above (pg. 2)\n",
    "            # Hint: how do we usually update weights? Use the true label somewhere...\n",
    "            if(prediction ==1 and y[i] == 0):\n",
    "                w = w - sample\n",
    "            elif(prediction == 0 and y[i]==1):\n",
    "                w = w+sample\n",
    "            else:\n",
    "                w = w\n",
    "            ##\n",
    "            \n",
    "            i += 1\n",
    "            # This will help us see how often we make mistakes\n",
    "            print(\"Weight vector on epoch \", j, \" sample \", i, \"is \", w)\n",
    "    return w\n",
    "\n",
    "# Now, run the perceptron! Remember, you are done when the weight vector stabilizes.\n",
    "epochs = 4 # \"stop criteria\" - arbitrary, feel free to change as you see fit.\n",
    "w_new = update(w, x, y, epochs)\n",
    "print(\"The final weight vector to model OR(x1,x2) with this data is: \", w_new)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** We just showed that a perceptron can model **OR**$(x_1, x_2)$ successfully. Prove that a perceptron can't model **XNOR**$(x_1, x_2)$. *Hint: think about linearity.*"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part b]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem 3: Neural Networks and the XNOR Problem\n",
    "\n",
    "So, the perceptron can't model the **XNOR**$(x_1, x_2)$ function. We now want to design a neural network (by hand) to solve the **XNOR** problem. \n",
    "\n",
    "**a.** Write the **XNOR** function in terms of the logical functions **OR**$(x_1,x_2)$, **AND**$(x_1,x_2)$, **NOR**$(x_1,x_2)$. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part a]`**"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**b.** We will now design a network to model this function, using the hyperbolic tangent ([$tanh$](http://reference.wolfram.com/language/ref/Tanh.html)) as the activation function in all of the nodes. The network will take two binary variables as input, and output 1 only when the inputs are both 0 or both 1.\n",
    "\n",
    "The $tanh$ function outputs [-1,+1], not our desired output of [0,1]. Thus, we have appropriately changed the OR node to take +1/-1 as inputs. Also, we have added an extra last layer to convert the final output from +1/-1 to 0/1.\n",
    "\n",
    "*Hint: assume that $tanh$ outputs -1 for any input $x\\leq -2$, +1 for any input $x\\geq 2$, and 0 for $x=0$.*\n",
    "\n",
    "<img src=\"xnor1.png\" style=\"height:130px;\"><img src=\"xnor2.png\" style=\"height:110px;\">\n",
    "\n",
    "What are the missing weights $a,b,c,d,e,f$ of the **OR**, **NAND**, **AND** and **CONVERT** subnetworks, respectively?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "**`[double click here to add a solution to part b]`**"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.1"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
